---
title: Agent | 我搭建了一个 24h 运行的 AI 助理来收集信息
date: 2024-05-12 16:17:42
tags: 
- 大模型
- Agent
---

## 从例子开始：寻找一篇为大模型设计专用编程语言的文章

我将该问题翻译为：

```
Can you find me some papers about designing a new programming language for LLM model?
```

### Perplexity.ai 的表现

Perplexity.ai 是 AI 搜索行业的龙头企业之一，其搜索服务分为普通模式和 Pro 模式，其中 Pro 模式更强大，但限制了免费体验的次数。

下面是普通模式的表现：

```
Unfortunately...
```

嗯，先帝创业未半而骈死于槽枥之间了。看看后面说了什么：

```
Unfortunately, I could not find any papers specifically about designing a new programming language for LLM models in the given search results.

... However, the search results do provide some relevant information that could be useful for designing a programming language for LLMs:

1. The paper "Low-code LLM: Graphical User Interface over Large Language Models" [4] introduces a framework for interacting with LLMs through a graphical user interface and low-code visual programming. This could potentially be extended or adapted to design a programming language for LLMs...
```

全回答中只有低代码论文勉强和「代码」和「大语言模型」相关，但显然不是给大语言模型用的编程语言。

如果使用 Pro 模式，则会得到：

```
The query you've posed about finding papers on designing new programming languages for Large Language Models (LLMs) is addressed by two significant contributions in the field:

1. **"Prompting Is Programming: A Query Language for Large Language Models"** - This paper introduces the concept of Language Model Programming (LMP), which extends the idea of prompting LLMs with a combination of text prompts and scripting...

2. **"Efficiently Programming Large Language Models using SGLang"** - This paper presents SGLang, a domain-specific language tailored for programming LLMs...
```

这里得到了两篇真正相关的论文，一篇是设计提示词的编程语言 LMP，一篇是编排大模型的 SGLang，效果比基础版好一些。

注意到 Pro 模式的步骤中有一步是谷歌搜索 `site:arxiv.org programming language for large language models`，搜索得到的前两个结果正是回答中推荐的两篇文章。但 Pro 模式亦止步于此。

### 我的 AI 助理的表现

我的 [本地搜索助理](https://github.com/info-gap/info-gap-server)（怎么没人 Star 呢？）用 1 小时得到了 18 篇可能相关的文章，其中多数文章也并不符合要求，但得到的部分文章比传统 AI 搜索更符合需求。下面的文章推荐按照生成时间排序，最先生成的在最上面：

- CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts
  - 不相关，但大模型把减少计算消耗和新编程语言联系在一起
- Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?
  - 不相关，但大模型幻觉出 LLM-Script
- Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers
  - 不相关，但大模型把框架理解为编程语言
- Learned harmonic mean estimation of the Bayesian evidence with normalizing flows
  - 不相关，但这是一篇纯数学文章，大模型认为这为编程语言奠定基础
- Quantum Communication and Mixed-State Order in Decohered Symmetry-Protected Topological States
  - 不相关，但大模型认为这为硬件实现提供基础
- SPML: A DSL for Defending Language Models Against Prompt Attacks
  - 和新编程语言、大模型同时相关，专用于抵御提示词攻击的编程语言
- Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows
  - 不相关，大模型也知道不直接相关，但还是推荐了
- TypeFly: Flying Drones with Large Language Model
  - 和新编程语言、大模型同时相关，让大模型能控制无人机
- What Algorithms can Transformers Learn? A Study in Length Generalization
  - 和新编程语言、大模型同时相关，并且大模型可以被解释为在使用这个编程语言
- AutoScrum: Automating Project Planning Using Large Language Models
  - 不相关，但大模型幻觉出 LLM-Script
- Prompting Is Programming: A Query Language for Large Language Models
  - 和新编程语言、大模型同时相关，前面 Perplexity.ai 推荐过
- Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons
  - 不相关，但大模型幻觉出 LLM-Script
- A Survey on Visualization Approaches in Political Science for Social and Political Factors: Progress to Date and Future Opportunities
  - 不相关，大模型没说相关，但还是推荐了
- A Controlled Experiment on the Energy Efficiency of the Source Code Generated by Code Llama
  - 不相关，但大模型幻觉出 LLM-Script
- Large Language Models Synergize with Automated Machine Learning
  - 不相关，但大模型幻觉出 LlamaScript
- Generating Situated Reflection Triggers about Alternative Solution Paths: A Case Study of Generative AI for Computer-Supported Collaborative Learning
  - 不相关，但这是一篇辅助思考的文章，大模型认为这可以辅助构思新编程语言
- AI Coders Are Among Us: Rethinking Programming Language Grammar Towards Efficient Code Generation
  - 和新编程语言、大模型同时相关，是对原有编程语言适配大模型的改良
- Evaluating LLMs for Hardware Design and Test
  - 不相关，但里面涉及 HDL（硬件设计语言）使得大模型认为相关

虽然大部分文章都不相关，但找到的相关文章也更多！要知道在「信息检索」的场景下，得到一些无关信息是无伤大雅的，但漏掉真正相关的信息则是大问题！

同时你会注意到，其实很多无关信息大模型本完全可以筛选掉，比如那些幻觉。**但是！** 我其实是做了筛选的，而且事实上有 90% 左右的无关文章会被筛掉，但有关文章被筛掉的概率几乎为 0。剩下的文章以无关文章为主，是因为关键词检索出来的无关文章太多了……

你可能注意到 SGLang 没有被推荐，这是因为我还没实现分页查询的功能（昨天才开始写这玩意的代码……），所以比较早的文章不会被查询到。这个问题会在后面被讨论。

你可能会发现，几篇相关的文章几乎是连在一起出现的，这是因为对于同一个「检索词想法」搜索到的论文会被放在一起被判断。好的检索词想法更容易带来相关的文章。

## 我是怎么做到的？

![架构图](info-gap/graph.svg)

### 通过头脑风暴产生搜索关键字

这个节点会源源不断地产生新的搜索想法。怎么确保产生的想法都是新的？

我在实践中发现，如果把已经生成过的关键字放到提示词里面，反而会让大模型倾向于重复已有的内容。只需把模型温度设置为 1，并在提示词中引导「要有创意」，便能获得足够多样的关键字。

### 关键字校验

一次正常的和大模型的对话大概长这样：

```
user:

你要搜索关于为大模型设计编程语言的 arXiv 论文，你会用什么关键字收集？

llama3-cn:

如果你想要在arXiv上搜索关于大型（或深度）学习模型设计和编程语言的研究论文，以下是一些可能有用的关键词：

1. Deep Learning
2. Large-scale Models
3. Model Design
4. Programming Languages
5. Neural Networks
6. Distributed Training
7. Scalable Models
8. Big Data
9. Parallel Computing
10. High-Performance Computing (HPC)
11. GPU Acceleration
12. TensorFlow
13. PyTorch
14. Keras
15. Model Optimization
16. Efficient Algorithms
17. Model Serving
18. Deployment Techniques

你可以使用这些关键词单独或组合起来进行搜索，以便找到与大型模型设计相关的论文。例如，如果你特别感兴趣的是TensorFlow，你可以将“TensorFlow”和其他上述关键词结合使用来过滤结果。
```

这个回答人能看得懂，但如果直接把这个回答交给搜索 arXiv 论文的程序，这个程序怎么知道实际上应该搜索什么关键词呢？即使知道搜索什么关键词，程序也需要能校验关键词的格式，如果错误的话则需要让大模型重新输出一份。

产生这个问题是因为，目前设定的大模型的输出类型是宽泛的「文本」，而不是具体的「合法的搜索关键字」。我使用了 [Instructor](https://python.useinstructor.com/) 库来解决这个问题，这个库允许我定义「合法的搜索关键字」需要满足的要求，即符合 arXiv 的检索格式，并要求大模型输出这一类型的回复。如果不满足，校验者则会先尝试修复格式，如果无法修复则会告诉大模型为什么不合法，并且请求重新生成。

校验者事实上也是大模型，这里就用到了多智能体的思想。

### 论文搜索

直接使用 [arxiv.py](https://github.com/lukasschwab/arxiv.py) 库即可。

我计划加入通过搜索引擎搜索的功能，但目前还在调研各 API 的价格和性能。由于 API 调用次数将较多，我希望能尽可能减少消费。

目前对于论文我暂未实现分页查询，因此每次只是查询某关键字下最近发表的 10 篇论文。下一次迭代中，我计划使用任务池架构，对于所有搜索关键字，由调度器根据其结果接纳率决定「翻页」还是「产生新的搜索想法」。

### 推荐卡片撰写

这里我令大模型尝试输出一份对「文章符合用户要求」的证明，包括相关段落、相关理由和相关度打分。其中，相关段落会被校验是否引自原始文章，相关理由则会被校验是否能证明文章和用户要求相关，并且不包含幻觉。正如前面提到，这种校验在大部分情况下有效，但仍有少量情况下无法检验出幻觉。

要解决这个问题，需要在完善提示工程和提高模型质量之间权衡。我可能会尝试以下方案：

- 允许用户为「文章符合要求」的证明添加更多字段，以加入更有针对性的校验
- 允许大模型生成「不推荐」的推荐卡片（虽然直接给推荐卡片加 `Optional` 没用）
- 完善提示工程……（这玩意没数据集，不太方便测试性能，只能自己感觉了）

## 如何开始使用？

### 找到大模型解决方案

随便找个和 OpenAI API 兼容的即可，不兼容的话用 [LiteLLM](https://github.com/BerriAI/litellm) 反向代理一下就行。找不到的话可以看看下面的建议：

如果你有一张显存在 4.7 GB 以上的显卡，可以通过 [Ollama](https://ollama.com/library/llama3) 下载 LLaMA3-8b 模型并在本地运行。此时，你的 `OPENAI_API_URL` 是 `http://localhost:11434/v1`，`OPENAI_API_KEY` 是任意。我的上述测试结果是使用该模型得到的。

如果你能接受 1 天约 10 元的 API 调用费，可以考虑使用 [深度求索](https://www.deepseek.com/zh) 平台提供的 deepseek-chat-v2 模型。该模型的价格为每 1,000,000 令牌 1 元钱。上述消费估计假设了 1 分钟发送 6 次 API 请求，但该速率只是我的直觉估计，我无法保证准确。此时，你的 `OPENAI_API_URL` 是 `https://api.deepseek.com/v1`，`OPENAI_API_KEY` 是控制台中显示的 API Key。

**但是，** 我测试发现该模型和 Instructor 库的相性并不好，经常给出错误的 JSON 格式导致不断重试，因此如果你需要使用本模型，可能会需要手动调整提示词。

### 本地部署

欢迎 Star [本仓库](https://github.com/info-gap/info-gap-server)！

本 AI 助理目前正在频繁开发的阶段，暂时没有封装 Docker 版本，建议通过克隆本仓库的方式部署。如果使用 [Miniconda](https://docs.anaconda.com/free/miniconda/index.html)，使用下面的命令来配置开发环境：

```
# 克隆仓库
git clone git@github.com:info-gap/info-gap-server.git
cd info-gap-server

# 安装依赖
conda env create -f environment.yml
conda activate info-gap

# 设置环境变量，记得根据实际情况修改
echo "OPENAI_API_URL=http://localhost:11434/v1" >> .env
echo "OPENAI_API_KEY=ollama" >> .env

# 运行程序，输出会放到 ./log/ 中
python main.py
```

亦可以手动安装需要的包，然后使用相似的方法部署：

```
# 克隆仓库
git clone git@github.com:info-gap/info-gap-server.git
cd info-gap-server

# 安装依赖
pip install instructor arxiv

# 设置环境变量，记得根据实际情况修改
echo "OPENAI_API_URL=http://localhost:11434/v1" >> .env
echo "OPENAI_API_KEY=ollama" >> .env

# 运行程序，输出会放到 ./log/ 中
python main.py
```

如果需要更新仓库，使用 `git pull` 即可。

如果部署遇到问题，请提交 [Issue](https://github.com/info-gap/info-gap-server/issues/new/choose)。

